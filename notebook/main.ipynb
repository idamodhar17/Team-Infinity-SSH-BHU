{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Libraries\n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "import nltk\n",
    "import spacy\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading dataset\n",
    "data = pd.read_csv(\"test.tsv\", sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unnamed: 0    0\n",
      "title         0\n",
      "text          0\n",
      "subject       0\n",
      "date          0\n",
      "label         0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Checking for missing values\n",
    "print(data.isnull().sum())\n",
    "data = data.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the unnecessary 'Unnamed: 0' column\n",
    "data = data.drop(columns=['Unnamed: 0'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing duplicate rows\n",
    "data = data.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\zanwa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\zanwa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\zanwa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Initializing neccessary NLP tools\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    if not isinstance(text, str):  # Ensure the input is a string\n",
    "        return \"\"\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    # Remove special characters and numbers\n",
    "    text = re.sub(r'[^a-z\\s]', '', text)\n",
    "    # Use spaCy's tokenizer\n",
    "    doc = nlp(text)\n",
    "    # Remove stopwords\n",
    "    tokens = [token.text for token in doc if token.text not in stop_words]\n",
    "    # Lemmatize tokens\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "    return ' '.join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# applying the cleaning function to title and text columns\n",
    "data['cleaned_title'] = data['title'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['cleaned_text'] = data['text'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               title  \\\n",
      "0  Conservatives Will HATE What Donald Trump Just...   \n",
      "1  Trump victory may create new tension between U...   \n",
      "2  WATCH: Hundreds of ILLEGAL ALIENS Storm Senate...   \n",
      "3  Democratic Senator Franken to resign: CNN, cit...   \n",
      "4  GANG OF DOMESTIC TERRORISTS Violently Attack L...   \n",
      "\n",
      "                                       cleaned_title  \\\n",
      "0  conservative hate donald trump said planned pa...   \n",
      "1  trump victory may create new tension u islam i...   \n",
      "2  watch hundred illegal alien storm senate build...   \n",
      "3  democratic senator franken resign cnn citing s...   \n",
      "4  gang domestic terrorist violently attack lone ...   \n",
      "\n",
      "                                                text  \\\n",
      "0  Donald Trump isn t exactly a stranger to makin...   \n",
      "1  Donald Trumpâ€™s U.S. election victory may creat...   \n",
      "2  A couple of quick questions come to mind when ...   \n",
      "3  U.S. Democratic Senator Al Franken will announ...   \n",
      "4  ***WARNING*** Violence is graphic***This Trump...   \n",
      "\n",
      "                                        cleaned_text  \n",
      "0  donald trump exactly stranger making large gro...  \n",
      "1  donald trump u election victory may create fre...  \n",
      "2  couple quick question come mind lawabiding ame...  \n",
      "3  u democratic senator al franken announce resig...  \n",
      "4  warning violence graphicthis trump supporter u...  \n"
     ]
    }
   ],
   "source": [
    "print(data[['title', 'cleaned_title', 'text', 'cleaned_text']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['title_length'] = data['cleaned_title'].apply(lambda x: len(x.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['text_length'] = data['cleaned_text'].apply(lambda x: len(x.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "data['title_sentiment'] = data['cleaned_title'].apply(lambda x: TextBlob(x).sentiment.polarity)\n",
    "data['text_sentiment'] = data['cleaned_text'].apply(lambda x: TextBlob(x).sentiment.polarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = data[['cleaned_title', 'cleaned_text', 'title_length', 'text_length']]\n",
    "y = data['label']\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(max_features=5000)\n",
    "# Adjust max_features based on your dataset size\n",
    "X_train_tfidf = vectorizer.fit_transform(X_train['cleaned_text'])\n",
    "X_val_tfidf = vectorizer.transform(X_val['cleaned_text'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.96      0.96       850\n",
      "           1       0.95      0.96      0.96       802\n",
      "\n",
      "    accuracy                           0.96      1652\n",
      "   macro avg       0.96      0.96      0.96      1652\n",
      "weighted avg       0.96      0.96      0.96      1652\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "model = RandomForestClassifier(random_state=42)\n",
    "model.fit(X_train_tfidf, y_train)\n",
    "\n",
    "y_pred = model.predict(X_val_tfidf)\n",
    "print(classification_report(y_val, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('train.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
